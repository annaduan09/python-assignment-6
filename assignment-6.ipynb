{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Predictive Modeling of Housing Prices in Philadelphia\n",
    "\n",
    "**Due date: Wednesday, 12/6 by the end of the day**\n",
    "\n",
    "\n",
    "Lectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We'll extend that analysis in this section by:\n",
    "\n",
    "- Optimizing our hyperparameters during the modeling process using cross-validation and a grid search\n",
    "- Testing the fairness of our model by calculating the intersection of the model error rate and poverty rate across neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling Philadelphia's Housing Prices and Algorithmic Fairness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data from the Office of Property Assessment\n",
    "\n",
    "Use the requests package to query the CARTO API for **single-family** property assessment data in Philadelphia for properties that had their **last sale during 2022**.\n",
    "\n",
    "Sources: \n",
    "- [OpenDataPhilly](https://www.opendataphilly.org/dataset/opa-property-assessments)\n",
    "- [Metadata](http://metadata.phila.gov/#home/datasetdetails/5543865f20583086178c4ee5/representationdetails/55d624fdad35c7e854cb21a4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "\n",
    "# Show all columns\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The API endpoint\n",
    "carto_api_endpoint = \"https://phl.carto.com/api/v2/sql\"\n",
    "\n",
    "# The query parameters\n",
    "params = {\n",
    "    \"q\": \"SELECT * FROM opa_properties_public WHERE category_code_description = 'SINGLE FAMILY' AND sale_date >= '2022-01-01' AND sale_date < '2022-12-31'\",\n",
    "    \"format\": \"geojson\",\n",
    "    \"skipfields\": \"cartodb_id\"\n",
    "}\n",
    "\n",
    "response = requests.get(carto_api_endpoint, params=params)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "homes = gpd.GeoDataFrame.from_features(features, crs=\"EPSG:4326\")\n",
    "homes_clean = homes.dropna(subset=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load data for census tracts and neighborhoods\n",
    "\n",
    "Load various Philadelphia-based regions that we will use in our analysis. \n",
    "\n",
    "- Census tracts can be downloaded from: [https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson](https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson)\n",
    "- Neighborhoods can be downloaded from:\n",
    "[https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson](https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neigh = gpd.read_file(\"https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\")\n",
    "tract = gpd.read_file(\"Census_Tracts_2010.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>STATEFP10</th>\n",
       "      <th>COUNTYFP10</th>\n",
       "      <th>TRACTCE10</th>\n",
       "      <th>GEOID10</th>\n",
       "      <th>NAME10</th>\n",
       "      <th>NAMELSAD10</th>\n",
       "      <th>MTFCC10</th>\n",
       "      <th>FUNCSTAT10</th>\n",
       "      <th>ALAND10</th>\n",
       "      <th>AWATER10</th>\n",
       "      <th>INTPTLAT10</th>\n",
       "      <th>INTPTLON10</th>\n",
       "      <th>LOGRECNO</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>009400</td>\n",
       "      <td>42101009400</td>\n",
       "      <td>94</td>\n",
       "      <td>Census Tract 94</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>366717</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.9632709</td>\n",
       "      <td>-075.2322437</td>\n",
       "      <td>10429</td>\n",
       "      <td>POLYGON ((-75.22927 39.96054, -75.22865 39.960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>009500</td>\n",
       "      <td>42101009500</td>\n",
       "      <td>95</td>\n",
       "      <td>Census Tract 95</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>319070</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.9658709</td>\n",
       "      <td>-075.2379140</td>\n",
       "      <td>10430</td>\n",
       "      <td>POLYGON ((-75.23536 39.96852, -75.23545 39.969...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>009600</td>\n",
       "      <td>42101009600</td>\n",
       "      <td>96</td>\n",
       "      <td>Census Tract 96</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>405273</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.9655396</td>\n",
       "      <td>-075.2435075</td>\n",
       "      <td>10431</td>\n",
       "      <td>POLYGON ((-75.24343 39.96230, -75.24339 39.962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>013800</td>\n",
       "      <td>42101013800</td>\n",
       "      <td>138</td>\n",
       "      <td>Census Tract 138</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>341256</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.9764504</td>\n",
       "      <td>-075.1771771</td>\n",
       "      <td>10468</td>\n",
       "      <td>POLYGON ((-75.17341 39.97779, -75.17386 39.977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>013900</td>\n",
       "      <td>42101013900</td>\n",
       "      <td>139</td>\n",
       "      <td>Census Tract 139</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>562934</td>\n",
       "      <td>0</td>\n",
       "      <td>+39.9750563</td>\n",
       "      <td>-075.1711846</td>\n",
       "      <td>10469</td>\n",
       "      <td>POLYGON ((-75.17313 39.97776, -75.17321 39.977...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID STATEFP10 COUNTYFP10 TRACTCE10      GEOID10 NAME10  \\\n",
       "0         1        42        101    009400  42101009400     94   \n",
       "1         2        42        101    009500  42101009500     95   \n",
       "2         3        42        101    009600  42101009600     96   \n",
       "3         4        42        101    013800  42101013800    138   \n",
       "4         5        42        101    013900  42101013900    139   \n",
       "\n",
       "         NAMELSAD10 MTFCC10 FUNCSTAT10  ALAND10  AWATER10   INTPTLAT10  \\\n",
       "0   Census Tract 94   G5020          S   366717         0  +39.9632709   \n",
       "1   Census Tract 95   G5020          S   319070         0  +39.9658709   \n",
       "2   Census Tract 96   G5020          S   405273         0  +39.9655396   \n",
       "3  Census Tract 138   G5020          S   341256         0  +39.9764504   \n",
       "4  Census Tract 139   G5020          S   562934         0  +39.9750563   \n",
       "\n",
       "     INTPTLON10 LOGRECNO                                           geometry  \n",
       "0  -075.2322437    10429  POLYGON ((-75.22927 39.96054, -75.22865 39.960...  \n",
       "1  -075.2379140    10430  POLYGON ((-75.23536 39.96852, -75.23545 39.969...  \n",
       "2  -075.2435075    10431  POLYGON ((-75.24343 39.96230, -75.24339 39.962...  \n",
       "3  -075.1771771    10468  POLYGON ((-75.17341 39.97779, -75.17386 39.977...  \n",
       "4  -075.1711846    10469  POLYGON ((-75.17313 39.97776, -75.17321 39.977...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Spatially join the sales data and neighborhoods/census tracts.\n",
    "\n",
    "Perform a spatial join, such that each sale has an associated neighborhood and census tract.\n",
    "\n",
    "**Note:** After performing the first spatial join, you will need to use the `drop()` function to remove the `index_right` column; otherwise an error will be raised on the second spatial join about duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train a Random Forest on the sales data\n",
    "\n",
    "In this step, you should follow the steps outlined in lecture to preprocess and train your model. We'll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\n",
    "\n",
    "**Preprocessing Requirements**\n",
    "- Trim the sales data to those sales with prices between $3,000 and $1 million\n",
    "- Set up a pipeline that includes both numerical columns and categorical columns\n",
    "- Include one-hot encoded variable for the *neighborhood* of the sale, **instead of ZIP code**. We don't want to include multiple location based categories, since they encode much of the same information.\n",
    "\n",
    "**Training requirements**\n",
    "- Use a 70/30% training/test split and predict the log of the sales price.\n",
    "- Use GridSearchCV to perform a k-fold cross validation that optimize *at least 2* hyperparameters of the RandomForestRegressor\n",
    "- After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\n",
    "\n",
    "**Note**: You don't need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Calculate the percent error of your model predictions for each sale in the test set\n",
    "\n",
    "Fit your best model and use it to make predictions on the test set.\n",
    "\n",
    "**Note:** This should be the percent error in terms of **sale price**. You'll need to convert if your model predicted the log of sales price!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Make a data frame with percent errors and census tract info for each sale in the test set\n",
    "\n",
    "Create a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- When using the \"train_test_split()\" function, the index of the test data frame includes the labels from the original sales data frame\n",
    "- You can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\n",
    "- Add a new column to this data frame holding the percent error data\n",
    "- Make sure to use the percent error and not the absolute percent error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Plot a map of the median percent error by census tract \n",
    "\n",
    "- You'll want to group your data frame of test sales by the `GEOID10` column and take the median of you percent error column\n",
    "- Merge the census tract geometries back in and use geopandas to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Compare the percent errors in Qualifying Census Tracts and other tracts \n",
    "\n",
    "[Qualifying Census Tracts](https://www.huduser.gov/portal/datasets/qct.html) are a poverty designation that HUD uses to allocate housing tax credits\n",
    "\n",
    "- I've included a list of the census tract names that qualify in Philadelphia\n",
    "- Add a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\n",
    "- Then, group by this new column and calculate the median percent error\n",
    "\n",
    "**You should find that the algorithm's accuracy is significantly worse in these low-income, qualifying census tracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct = ['5',\n",
    " '20',\n",
    " '22',\n",
    " '28.01',\n",
    " '30.01',\n",
    " '30.02',\n",
    " '31',\n",
    " '32',\n",
    " '33',\n",
    " '36',\n",
    " '37.01',\n",
    " '37.02',\n",
    " '39.01',\n",
    " '41.01',\n",
    " '41.02',\n",
    " '56',\n",
    " '60',\n",
    " '61',\n",
    " '62',\n",
    " '63',\n",
    " '64',\n",
    " '65',\n",
    " '66',\n",
    " '67',\n",
    " '69',\n",
    " '70',\n",
    " '71.01',\n",
    " '71.02',\n",
    " '72',\n",
    " '73',\n",
    " '74',\n",
    " '77',\n",
    " '78',\n",
    " '80',\n",
    " '81.01',\n",
    " '81.02',\n",
    " '82',\n",
    " '83.01',\n",
    " '83.02',\n",
    " '84',\n",
    " '85',\n",
    " '86.01',\n",
    " '86.02',\n",
    " '87.01',\n",
    " '87.02',\n",
    " '88.01',\n",
    " '88.02',\n",
    " '90',\n",
    " '91',\n",
    " '92',\n",
    " '93',\n",
    " '94',\n",
    " '95',\n",
    " '96',\n",
    " '98.01',\n",
    " '100',\n",
    " '101',\n",
    " '102',\n",
    " '103',\n",
    " '104',\n",
    " '105',\n",
    " '106',\n",
    " '107',\n",
    " '108',\n",
    " '109',\n",
    " '110',\n",
    " '111',\n",
    " '112',\n",
    " '113',\n",
    " '119',\n",
    " '121',\n",
    " '122.01',\n",
    " '122.03',\n",
    " '131',\n",
    " '132',\n",
    " '137',\n",
    " '138',\n",
    " '139',\n",
    " '140',\n",
    " '141',\n",
    " '144',\n",
    " '145',\n",
    " '146',\n",
    " '147',\n",
    " '148',\n",
    " '149',\n",
    " '151.01',\n",
    " '151.02',\n",
    " '152',\n",
    " '153',\n",
    " '156',\n",
    " '157',\n",
    " '161',\n",
    " '162',\n",
    " '163',\n",
    " '164',\n",
    " '165',\n",
    " '167.01',\n",
    " '167.02',\n",
    " '168',\n",
    " '169.01',\n",
    " '169.02',\n",
    " '170',\n",
    " '171',\n",
    " '172.01',\n",
    " '172.02',\n",
    " '173',\n",
    " '174',\n",
    " '175',\n",
    " '176.01',\n",
    " '176.02',\n",
    " '177.01',\n",
    " '177.02',\n",
    " '178',\n",
    " '179',\n",
    " '180.02',\n",
    " '188',\n",
    " '190',\n",
    " '191',\n",
    " '192',\n",
    " '195.01',\n",
    " '195.02',\n",
    " '197',\n",
    " '198',\n",
    " '199',\n",
    " '200',\n",
    " '201.01',\n",
    " '201.02',\n",
    " '202',\n",
    " '203',\n",
    " '204',\n",
    " '205',\n",
    " '206',\n",
    " '208',\n",
    " '239',\n",
    " '240',\n",
    " '241',\n",
    " '242',\n",
    " '243',\n",
    " '244',\n",
    " '245',\n",
    " '246',\n",
    " '247',\n",
    " '249',\n",
    " '252',\n",
    " '253',\n",
    " '265',\n",
    " '267',\n",
    " '268',\n",
    " '271',\n",
    " '274.01',\n",
    " '274.02',\n",
    " '275',\n",
    " '276',\n",
    " '277',\n",
    " '278',\n",
    " '279.01',\n",
    " '279.02',\n",
    " '280',\n",
    " '281',\n",
    " '282',\n",
    " '283',\n",
    " '284',\n",
    " '285',\n",
    " '286',\n",
    " '287',\n",
    " '288',\n",
    " '289.01',\n",
    " '289.02',\n",
    " '290',\n",
    " '291',\n",
    " '293',\n",
    " '294',\n",
    " '298',\n",
    " '299',\n",
    " '300',\n",
    " '301',\n",
    " '302',\n",
    " '305.01',\n",
    " '305.02',\n",
    " '309',\n",
    " '311.01',\n",
    " '312',\n",
    " '313',\n",
    " '314.01',\n",
    " '314.02',\n",
    " '316',\n",
    " '318',\n",
    " '319',\n",
    " '321',\n",
    " '325',\n",
    " '329',\n",
    " '330',\n",
    " '337.01',\n",
    " '345.01',\n",
    " '357.01',\n",
    " '376',\n",
    " '377',\n",
    " '380',\n",
    " '381',\n",
    " '382',\n",
    " '383',\n",
    " '389',\n",
    " '390']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
